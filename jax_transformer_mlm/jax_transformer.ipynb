{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be55fc4-3a68-4413-92e8-f7959a6212ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "parent = dirname(dirname(abspath(\"__file__\")))\n",
    "sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc64c6e-6ab2-4400-98f8-ee7b82aac248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 10:27:11.992053: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-15 10:27:11.992077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-15 10:27:11.993010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-15 10:27:11.997851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-15 10:27:12.633457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7768a065-896d-443e-af81-60d62ea4465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c079e27-a9a2-431a-ba43-5bff8c045b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "main_rng = jax.random.PRNGKey(421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff5abe1-7c24-49d8-91bf-4b4286a398f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.training import checkpoints\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344c5606-6d87-41a6-b576-809719ca9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_display import nice_colorbar\n",
    "from utils_display import pc\n",
    "from jax_transformer_display_helpers import display_scaled_dot_product\n",
    "from jax_transformer_display_helpers import display_positional_encoding\n",
    "from jax_transformer_display_helpers import display_positional_encoding_profiles\n",
    "from jax_transformer_display_helpers import display_lr_scheduler\n",
    "\n",
    "from trainer import Trainer\n",
    "from transformer_predictor import TransformerPredictor\n",
    "\n",
    "from transformer_helpers import MultiheadAttention\n",
    "from transformer_helpers import EncoderBlock\n",
    "from transformer_helpers import TransformerEncoder\n",
    "from transformer_helpers import scaled_dot_product\n",
    "from transformer_helpers import expand_mask\n",
    "from transformer_helpers import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54efa7fd-da82-42c9-8d2a-1979542e9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/media/guillaume/DATA/NERD/GitHub/nlp/jax_transformer_mlm/jax_checkpoints/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695f1de-917f-4dab-a688-72063d15fee0",
   "metadata": {},
   "source": [
    "# Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592b2de8-26fc-4721-a636-ba9b01ac77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 5\n",
    "embedding_dimensionality = 3\n",
    "\n",
    "_, rand1 = jax.random.split(main_rng)\n",
    "\n",
    "qkv = jax.random.normal(rand1, (3, sequence_length, embedding_dimensionality))\n",
    "q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "mask = jnp.zeros((sequence_length, sequence_length))\n",
    "mask = mask.at[3,:].set(1)\n",
    "\n",
    "weighted_sum_of_values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
    "                 \n",
    "## display_scaled_dot_product(q, k, v, mask, weighted_sum_of_values, attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322d373-cdfd-4b85-83f8-fc5e72f5c6fc",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2064080-080f-4970-aa68-e61b4c0c5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (2, 13, 32) Attention (2, 4, 13, 13)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "batch_size = 2\n",
    "sequence_length = 13\n",
    "embedding_dimensionality = 32\n",
    "number_of_heads = 4\n",
    "\n",
    "main_rng, x_rng = jax.random.split(main_rng)\n",
    "\n",
    "x = jax.random.normal(x_rng, (batch_size, sequence_length, embedding_dimensionality))\n",
    "\n",
    "mha = MultiheadAttention(embedding_dimensionality=embedding_dimensionality, number_of_heads=number_of_heads)\n",
    "\n",
    "main_rng, init_rng = jax.random.split(main_rng)\n",
    "\n",
    "params = mha.init(init_rng, x)['params']\n",
    "w_o, attention_weights = mha.apply({'params': params}, x)\n",
    "\n",
    "print('Out', w_o.shape, 'Attention', attention_weights.shape)\n",
    "\n",
    "del w_o, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6782902-5177-4bf3-9be9-334335c0ba61",
   "metadata": {},
   "source": [
    "# Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509dc391-4285-421d-866a-b7ca98197d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "main_rng, x_rng = jax.random.split(main_rng)\n",
    "x = jax.random.normal(x_rng, (3, 16, 128))\n",
    "\n",
    "encblock = EncoderBlock(input_dimensionality=128, number_of_heads=4, feedforward_dimensionality=512, dropout_probability=0.1)\n",
    "\n",
    "main_rng, init_rng, dropout_init_rng = jax.random.split(main_rng, 3)\n",
    "params = encblock.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "main_rng, dropout_apply_rng = jax.random.split(main_rng)\n",
    "out = encblock.apply({'params': params}, x, train=True, rngs={'dropout': dropout_apply_rng})\n",
    "print('Out', out.shape)\n",
    "\n",
    "del encblock, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a7772f-a896-4cd2-86e6-99d1f318a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out (3, 16, 128)\n",
      "Attention maps 5 (3, 4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "main_rng, x_rng = jax.random.split(main_rng)\n",
    "x = jax.random.normal(x_rng, (3, 16, 128))\n",
    "\n",
    "transenc = TransformerEncoder(\n",
    "    number_of_layers=5,\n",
    "    input_dimensionality=128,\n",
    "    number_of_heads=4,\n",
    "    feedforward_dimensionality=256,\n",
    "    dropout_probability=0.15)\n",
    "\n",
    "main_rng, init_rng, dropout_init_rng = jax.random.split(main_rng, 3)\n",
    "params = transenc.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = jax.random.split(main_rng)\n",
    "\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "binded_mod = transenc.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "out = binded_mod(x, train=True)\n",
    "print('Out', out.shape)\n",
    "\n",
    "attn_maps = binded_mod.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transenc, binded_mod, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5138c63-d1b4-4c78-b207-cee6d551265a",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8a9b7a-fe79-47c2-a019-0a14eef1c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_block = PositionalEncoding(hidden_dimensionality=48, maximum_sequence_length=96).bind({})\n",
    "positional_encoding = jax.device_get(encod_block.positional_encoding.squeeze().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd23576f-0b72-40c6-9269-6188fc724205",
   "metadata": {},
   "outputs": [],
   "source": [
    "## display_positional_encoding(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c847c3d-99ce-4312-b1de-07a47b6d56f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## display_positional_encoding_profiles(positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884dcfac-a44f-495d-9d80-c806e1f89f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e138d08-4e4d-43f7-827a-ea078afc3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_warmup_schedule(base_lr: float, warmup: int, max_iters: int):\n",
    "    assert warmup > 0 and max_iters > 0\n",
    "    # Create function to return lr based on iteration count\n",
    "    def get_lr(train_iter):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * train_iter / max_iters))\n",
    "        if train_iter <= warmup:\n",
    "            lr_factor *= train_iter * 1.0 / warmup\n",
    "        return lr_factor * base_lr\n",
    "    return get_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61738839-ab4f-4934-849a-2c4fc21414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = cosine_warmup_schedule(base_lr=1.0, warmup=100, max_iters=2000)\n",
    "\n",
    "## display_lr_scheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1c965-287b-4717-8134-ac9f9fb5a28a",
   "metadata": {},
   "source": [
    "# Full transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a0d7a6-b150-4db9-9813-f4a98f5ab480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[initialization finished]\n",
      "Out (3, 17, 17)\n",
      "Attention maps 5 (3, 64, 17, 17)\n"
     ]
    }
   ],
   "source": [
    "main_rng, x_rng = jax.random.split(main_rng)\n",
    "\n",
    "x = jax.random.normal(x_rng, (3, 17, 1))\n",
    "\n",
    "transpre = TransformerPredictor(\n",
    "    num_layers=5,\n",
    "    model_dim=128,\n",
    "    num_classes=17,\n",
    "    num_heads=64,\n",
    "    dropout_prob=0.15,\n",
    "    input_dropout_prob=0.05)\n",
    "\n",
    "# Initialize parameters of transformer predictor with random key and inputs\n",
    "main_rng, init_rng, dropout_init_rng = jax.random.split(main_rng, 3)\n",
    "params = transpre.init({'params': init_rng, 'dropout': dropout_init_rng}, x, train=True)['params']\n",
    "\n",
    "print('[initialization finished]')\n",
    "\n",
    "# Apply transformer predictor with parameters on the inputs\n",
    "# Since dropout is stochastic, we need to pass a rng to the forward\n",
    "main_rng, dropout_apply_rng = jax.random.split(main_rng)\n",
    "\n",
    "# Instead of passing params and rngs every time to a function call, we can bind them to the module\n",
    "model = transpre.bind({'params': params}, rngs={'dropout': dropout_apply_rng})\n",
    "\n",
    "\n",
    "out = model(x, mask=None, add_positional_encoding=True, train=True)\n",
    "print('Out', out.shape)\n",
    "\n",
    "\n",
    "attn_maps = model.get_attention_maps(x, train=True)\n",
    "print('Attention maps', len(attn_maps), attn_maps[0].shape)\n",
    "\n",
    "del transpre, model, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dde69-4b1b-4a52-ae60-499616b5ba50",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e564d8-48df-42df-86b3-b28499bd9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, np_rng):\n",
    "        super().__init__()\n",
    "\n",
    "        with open(os.path.join(\"..\", \"local_datasets\", \"wikipedia_man_o_war.pkl\"), \"rb\") as fid:                                       \n",
    "            self.dico_word2index, self.dico_index2word, self.dataset = pickle.load(fid) \n",
    "        \n",
    "        self.num_categories = len(self.dico_word2index)\n",
    "        self.maximum_sequence_length = len(self.dataset[0][\"mask\"])\n",
    "        self.size = len(self.dataset)\n",
    "        self.np_rng = np_rng\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_indices = self.dataset[index][\"input_indices\"]\n",
    "        mask = self.dataset[index][\"mask\"]\n",
    "        masked_indices = self.dataset[index][\"masked_indices\"]\n",
    "        labels = self.dataset[index][\"labels\"]\n",
    "        return input_indices, mask, masked_indices, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0e01d1-c973-4275-806a-778413e3dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "    \"\"\"\n",
    "    u = np.array(batch)\n",
    "    return u[:, 0, :, None], u[:, 1, :, None], u[:, 2, :, None], u[:, 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af9a7af7-6a32-410f-9490-d7a1405bd642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNumber of categories\u001b[0m: 1099\n",
      "\u001b[34mMaximum sequence length\u001b[0m: 50\n",
      "\u001b[34mDataset size\u001b[0m: 132\n",
      "\u001b[34mSequence length\u001b[0m: 50\n",
      "\u001b[34mInput indices\u001b[0m: [ 59  30  14 152 265   3 266  39  10  55 153   4  26  27  48   4   7 189\n",
      "   4  26  27 154 155   4 267 190 268 456  49  60 156   5   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
      "\u001b[34mMask\u001b[0m: [0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\u001b[34mMasked indices\u001b[0m: [ 59   0  14   0 265   0 266  39  10  55 153   4  26  27   0   4   7 189\n",
      "   4  26   0 154 155   0 267 190 268 456  49  60 156   5   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   1   1]\n",
      "\u001b[34mLabels\u001b[0m: [-100   30 -100  152 -100    3 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "   48 -100 -100 -100 -100 -100   27 -100 -100    4 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      " -100 -100 -100 -100 -100 -100 -100 -100]\n"
     ]
    }
   ],
   "source": [
    "np_rng = np.random.default_rng(421)\n",
    "\n",
    "dataset = MLMDataset(np_rng=np_rng)\n",
    "\n",
    "data_loader_train = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=9,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=numpy_collate)\n",
    "\n",
    "data_loader_validation = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate)\n",
    "\n",
    "data_loader_test = data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=numpy_collate)\n",
    "\n",
    "number_of_categories = dataset.num_categories\n",
    "\n",
    "pc(\"Number of categories\", number_of_categories)\n",
    "pc(\"Maximum sequence length\", dataset.maximum_sequence_length)\n",
    "pc(\"Dataset size\", dataset.size)\n",
    "\n",
    "index = 2\n",
    "input_indices, mask, masked_indices, labels = data_loader_train.dataset[index]\n",
    "\n",
    "pc(\"Sequence length\", len(input_indices))\n",
    "pc(\"Input indices\", input_indices)\n",
    "pc(\"Mask\", mask)\n",
    "pc(\"Masked indices\", masked_indices)\n",
    "pc(\"Labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6971a458-8e0d-4a7d-885b-c90c436bd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(9, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "u = next(iter(data_loader_train))\n",
    "print(len(u))\n",
    "print(u[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e08b8fa0-d718-45dc-a057-1c300c37c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(max_epochs=10, **model_args):\n",
    "    number_of_iterations = len(data_loader_train) * max_epochs\n",
    "\n",
    "    exmp_batch,  _,  _, _ = next(iter(data_loader_train))\n",
    "    trainer = Trainer(\n",
    "        model_name=\"Trainer\",\n",
    "        exmp_batch=exmp_batch,\n",
    "        max_iters=number_of_iterations,\n",
    "        checkpoint_path=CHECKPOINT_PATH,\n",
    "        **model_args)\n",
    "\n",
    "    if not trainer.checkpoint_exists():\n",
    "        trainer.train_model(data_loader_train, data_loader_validation, num_epochs=max_epochs)\n",
    "        trainer.load_model()\n",
    "    else:\n",
    "        trainer.load_model(pretrained=True)\n",
    "        \n",
    "    val_acc = trainer.eval_model(data_loader_validation)\n",
    "    test_acc = trainer.eval_model(data_loader_test)\n",
    "    \n",
    "    # Bind parameters to model for easier inference\n",
    "    trainer.model_bd = trainer.model.bind({\"params\": trainer.state.params})\n",
    "    return trainer, {\"val_acc\": val_acc, \"test_acc\": test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5d6ada7-1bac-4251-945a-520d1767ebbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e2e4e54a214950a6bf4c4e5e5b77ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1e3a67584b41bca0a72e61e27c7c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c658f522ac4357a2076baf8d85c3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12509e87c7c14d9f9a8cf6426f5d40e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd5382869b646999a3f6668932eb720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ab88ddae5e4a988f38464d5191ea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4194b6771d574ef299c40245d485d6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3101ae16fe6844cbb811d70bf478f749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1a08a6e4b8453fa7339fe2b53fb28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaf381261b84d959f093942f3deea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642518e2a2144d5ab0acf991b9cd75d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.\n"
     ]
    }
   ],
   "source": [
    "reverse_trainer, reverse_result = do_training(\n",
    "    model_dim=128,\n",
    "    num_classes=number_of_categories,\n",
    "    num_heads=2,                                                \n",
    "    num_layers=3,\n",
    "    dropout_prob=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb1adc-dc82-46f1-9865-b634999d8af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
